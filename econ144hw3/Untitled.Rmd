---
title: "econ144hw3"
author: "Sijia Hua"
date: "5/6/2019"
output:
  html_document: default
  pdf_document: default
---

```{r setup}
library(marima)
library(strucchange)
library(seasonal)
library(dynlm)
library(gdata)
require(graphics)
library("readxl")
library('xts')
library('forecast');
library('fma')
library('expsmooth')
library('lmtest')
library('tseries')
library('Quandl')
library('fpp');
library('urca')
library(Hmisc)
setwd("/Users/Renaissance/Desktop/econ144/econ144hw3")
```

## first question 
```{r 1}
hw31 <- read.table("w-gs1yr.txt")
inte=interpNA(hw31$V4,method="linear")
inte2 <- as.numeric(inte)
## a 
t1<-ts(inte,start=1962,2009,freq = 52)
plot(t1,ylab="rate",main="U.S. weekly Interest Rate time series plot")
tsdisplay(inte2,main = "U.S. weekly Interest Rate time series plot")
# ACF of this dataset is graudally decreasing in a very slow trend, in PACF graph there is only first two spikes that are of significance.  I would interpret it as a ar(2) model, even the trend of acf is unclear.
## b 
# try ar2
t12<-as.numeric(t1[2:2445])
ar2_q1 <- arma(t12,order=c(2,0))
summary(ar2_q1)
# try ar1 +ma2
ar1ma2_q1 <- arma(t12,order=c(1,2))
summary(ar1ma2_q1)
# ar2 +ma1
ar2ma1_q1 <- arma(t12,order=c(2,1))
summary(ar2ma1_q1)

plot(t12,type='l',xlim=c(1000,1100),lwd=2)
lines(ar2_q1$fitted.values,type='l',col='yellow',lwd=2)
lines(ar1ma1_q1$fitted.values,type='l',col='red',lwd=2)
lines(ar2ma1_q1$fitted.values,type='l',col='green',lwd=2)
legend("topright",pch= c("-","-","-","-"),legend=c("original data", "ar2 fit","ar1ma2 fit","ar2ma1 fit"),col=c("yellow", "blue","red","green"),  cex=0.8)
# I prefer arma(2,1) model. Because as it is shown in the sumamry, all coefficients are of significant. It also gives us the lowest AIC value. 
## c
Acf(ar2ma1_q1$residuals)
Pacf(ar2ma1_q1$residuals)
# In acf and pacf graph, there are some residuals of significance. Hence arma(2,1) may not be a good fit.
## d
rec_q1=recresid(ar2ma1_q1$residuals~1)
plot(rec_q1, pch=16,ylab="Recursive Residuals")
# residuals are bounced around 0 randomly. But there are large residuals around index= 1000.
## e
plot(efp(ar2ma1_q1$residuals~1, type = "Rec-CUSUM"))
# this series passes the recursive cusum test. 
## f
auto_p1 <- auto.arima(t12)
summary(auto_p1)
# the best fit in r is arima(1,1,2): that is ar order = 1, level of differencing = 1, ma order = 2
plot(t12,type='l',xlim=c(500,600),lwd=2)
lines(ar2ma1_q1$fitted.values,type='l',col='green',lwd=2)
lines(auto_p1$fitted,type='l',col='red',lwd=2)
# these two simulations coincide each other. 
## g
f_q1 = Arima(t12,order=c(2,0,1))
plot(forecast(f_q1,h=24),shadecols="oldstyle",xlim=c(2000,2500))
plot(forecast(auto_q1,h=24),shadecols="oldstyle",xlim=c(2000,2500))
# arima(2,0,1) gives a upward trend approxiamtion, while arima(1,1,2) generated by autoarima gives a relatively flat approximation. 
```


## 6.5 from book a
```{r 6.5a}
# generate data set in 6.4
ma2 <- arima.sim(model=list(ma=c(-2,1.35)),n=100)+0.7
plot(ma2)
Acf(ma2)
Pacf(ma2)

## a
ma2_summary=arma(ma2,order=c(0,2)) 
summary(ma2_summary)
# We can see that ma1, ma2 and intercept are all of significant, so this is a ma2 process. 

ma2_sim <- Arima(ma2,order=c(0,0,2),include.drift=TRUE)
plot(ma2,lwd=2)
lines(ma2_sim$fitted,col="blue",lwd=2)
legend("topright",pch= c("-","-"),legend=c("Generated ma2", "ma2 simulation"),col=c("black", "blue"),  cex=0.8)
# From the graph we can see that simulated ma2 model is very close to the theoretical model, but the magnitide of original data is larger or equal to the fitted one. 

## b
# 1-step ahead
plot(forecast(ma2_sim,h=1),shadecols="oldstyle")
# 2-step ahead
plot(forecast(ma2_sim,h=2),shadecols="oldstyle")
# 3-step ahead
plot(forecast(ma2_sim,h=3),shadecols="oldstyle")
```

## 6.6 from book a
```{r 6.6a}
# first
y1 <- arima.sim(model = list(ma=c(0.8)),n=100)+1.2
# autoregressive: 
# Y_t = 1.2 + 0.8*(Y_t-1 - 1.2) - 0.8^2*(Y_t-2 - 1.2) +... + (-1)^t*0.8^(t-1)*(Y_1 - 1.2) + e_t

# second
y2 <- arima.sim(model = list(ma=c(1.25)),n=100)+1.2
# autoregressive:
# -1/0.8 * Y_t+1 = 1/0.8^2 * Y_t+2 + 1/0.8^3 * Y_t+3 +... + e_t

# plot
plot(y1)
plot(y2)
Acf(y1)
Acf(y2)
Pacf(y1)
Pacf(y2)

# I prefer the first model. Because its invertibility, we can convert MA(1) process into its own past, while the autoregressive process of the second model consists of its future values(it doesn't make sense). Hence, for the forecasting purpose, I will use model 1. l 
```

## 7.6 from book a
```{r 7.6a}
hw31 <- read.xls('hw37a.xls',sheet = 3)
# delete na
hw31 <- hw31[-c(1),]
# house inflation
hinf <-hw31$housing.Inflation....
thinf<-ts(hinf,start=1968,2011,freq = 1)
tsdisplay(thinf,ylab="House Inflation Rate",main="House inflation rate time series")
# from acf and pacf, housing inflation rate can be modeled as ar2 
ar21 <- arma(hinf,order=c(2,0)) #Same as MA(1) = AR(0) + MA(1)
summary(ar21)
# from the t test, all coefficients are of significances. Hence house inflation rate can be modeled as an ar2 process.

# transportation inflation
tinf <-hw31$transportation.Inflation....
ttinf<-ts(tinf,start=1968,2011,freq = 1)
tsdisplay(ttinf,ylab="Transport Inflation Rate",main="Transport inflation rate time series")
# from acf and pacf, transport inflation rate can not be modeled as ar2
ar22 <- arma(tinf,order=c(2,0))
summary(ar22)
# from the t test, coefficient of ar2 is not of significance. Hence transportation inflation rate can not be modeled as an ar2 process.

```

## 7.7 from book a
```{r 7.7a}
hw32<-read.xls('hw37a.xls',sheet = 4)
hw32<-hw32[-c(1),] # delete na row

# food inflation
finf <- hw32$food.Inflation....
tfinf <- ts(finf,1958,2011,freq=1)
tsdisplay(tfinf)  
ffit <- auto.arima(finf)
summary(ffit) # ar(1)+ma(1) process

ffit2 <- arma(finf,order=c(1,0))# try ar1
summary(ffit2) # compare aic and find ffit works better
ffitr = Arima(finf,order=c(1,0,1),include.drift=TRUE) # (1,1) fit
# forecast
p1 <-forecast(ffit,h=1) # 1 step ahead
p2 <-forecast(ffit,h=2) # 2 step ahead
p3 <-forecast(ffit,h=3) # 3 step ahead
# forecast error
e1 <- recresid(ffitr$res~1)
e1
# forecast uncertainty
# sigma^2 is 5.353

# gas inflation
ginf <- hw32$Gas.Inflation....
tginf <-ts(ginf,1958,2011,freq=1)
tsdisplay(tginf) # white noise
gfit <- auto.arima(ginf)
summary(gfit) # ma1 process

gfit2 <- arma(ginf,order=c(1,1)) # try ar1 +ma1 process
summary(gfit2)  # compare aic and find gfit works better
# forecast
p21 <-forecast(ffit,h=1) # 1 step ahead
p22 <-forecast(ffit,h=2) # 2 step ahead
p23 <-forecast(ffit,h=3)#  3 step ahead
# forecast error
e2 <- recresid(gfit$residuals~1)
e2
# forecast uncertainty
# sigma^2  is 144.5

```

## 6.2 from book c
```{r  6.2c}
## a
plot(plastics)
# There is a seasonal cycle in between each unit time. A peak appears in the middle of the unit time. The overall trend is upward increasing. 
## b 
plasticts<-ts(plastics,1,5,frequency = 12)
plasticts %>% decompose(type="multiplicative") %>%autoplot()
splastics<-decompose(plasticts,"multiplicative")
stlplastic <- stl(plasticts, s.window = "periodic")
# trend is in a linear growth 
stlplastic # seasonal indices

## c
# Yes, it supports.

## d
adplastics <- plasticts/splastics$seasonal
plot(adplastics) # adjust plastics data

## e
p2<-plastics
p2[20]<-plastics[20]+500
p2ts<-ts(p2,1,5,frequency = 12)
p2ts %>% decompose(type="multiplicative") %>%autoplot()
psd<-decompose(p2ts,type="multiplicative")
adp2<-seasadj(psd)
plot(adp2)
# In e, the new outlier drives the seasonal adjusted time series to have a spike at the point we add 500. 
## f
p3<-plastics
p3[40]<-plastics[40]+500
p3ts<-ts(p3,1,5,frequency = 12)
psd3 <- decompose(p3ts, type='multiplicative')
adp3 <- seasadj(psd3)
plot(adp3)
# no matter where the outlier exists, it will always generate similar spike but at different location, while other parts give the same result. 

```

## 8.6 from book c
```{r 8.6c}
## a
# generate ar(1)
y <- ts(numeric(100))
e <- rnorm(100)
for(i in 2:100)
  y[i] <- 0.6*y[i-1] + e[i]
## b
plot(y)
# as I change the value of ϕ1, the graph changes. As ϕ1 becomes larger, the graph illustrates more persistence. 
## c
# generate ma(1)
yma <- ts(numeric(100))
ema <- rnorm(100)
for (i in 2:100) 
  yma[i] <- 0.6*ema[i-1]+ema[i]
## d 
plot(yma)
#  as I change theta 1 in ma(1) model, the graph changes. As thta 1 becomes smaller, the frequency of fluctuation has increased. 
## e
# generate arma(1,1)
yarma <- ts(numeric(100))
earma <- rnorm(100)
for (i in 2:100) 
  yarma[i] <- 0.6*y[i-1]+0.6*earma[i-1]+earma[i]
## f
# generate ar(2)
yar2 <- ts(numeric(100))
ear2 <- rnorm(100)
for(i in 3:100)
  yar2[i] <- -0.8*yar2[i-1] +0.3*yar2[i-2]+ear2[i]
## g
# graph last two and compare
tsdisplay(yarma)
tsdisplay(yar2)

# The oscilation magnitude increases as time increases in ar2 model, which is a non-stationary process. ARMA(1,1) is considered to be stationary.  
```

















